{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.losses import mse\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_multivariate_normal_keras(mu_p, log_var_p, mu_q, log_var_q):\n",
    "    sigma_p = K.exp(log_var_p)            ## log variance to variance\n",
    "    sigma_q = K.exp(log_var_q)\n",
    "\n",
    "    term1 = K.sum(log_var_p - log_var_q)\n",
    "\n",
    "    term2 = K.sum(sigma_q/sigma_p)\n",
    "\n",
    "    term3 = K.sum(K.square(mu_q - mu_p) / sigma_p)\n",
    "\n",
    "    return (term1 + term2 + term3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js_divergence_multivariate_normal_keras(mu_post, log_var_post, inputs_k, js_param):\n",
    "    var_post = K.exp(log_var_post)            ## log variance to variance\n",
    "\n",
    "    term1 = K.sum(log_var_post)\n",
    "    term2 = K.sum(K.square(mu_post - inputs_k) / var_post)\n",
    "\n",
    "    return js_param*(term1 + term2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(alpha_js,inputs_k_pr, inputs_k,z_mean,z_log_var):\n",
    "    js_param = (1.0-alpha_js)/alpha_js\n",
    "    #reconstruction_loss = mse(inputs_x,outputs)\n",
    "    kl_loss = (1.0/800.0)*kl_divergence_multivariate_normal_keras(inputs_k_pr, K.constant([0.0]), z_mean, z_log_var)\n",
    "    label_loss = (1.0/800.0)*js_divergence_multivariate_normal_keras(z_mean,z_log_var,inputs_k,js_param)\n",
    "\n",
    "\n",
    "    return  kl_loss + label_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model(hp):   \n",
    "#     input_shape_k = (1, )\n",
    "#     input_shape_x = (10,)\n",
    "#     latent_dim = 1\n",
    "    \n",
    "#     inputs_k = Input(shape=input_shape_k, name='ground_truth')\n",
    "#     inputs_k_pr = Input(shape=input_shape_k, name = 'prior mean')\n",
    "#     # latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "#     # intermediate_dim_1 = 8 # 1st layer\n",
    "#     intermediate_dim_2 = 5 # 2nd Layer\n",
    "#     intermediate_dim_3 = 3 # 3rd Layer\n",
    "\n",
    "\n",
    "#     inputs_x = Input(shape=input_shape_x, name='temp_input')\n",
    "#     inter_x1 = Dense(hp.Choice('units',[9,8,7,6]), activation='tanh', name='encoder_intermediate_1')(inputs_x)\n",
    "#     inter_x2 = Dense(intermediate_dim_2, activation='tanh', name='encoder_intermediate_2')(inter_x1)\n",
    "#     inter_x3 = Dense(intermediate_dim_3, activation='tanh', name='encoder_intermediate_3')(inter_x2)\n",
    "\n",
    "# # q(z|x)\n",
    "#     z_mean = Dense(latent_dim, name='z_mean')(inter_x3)\n",
    "#     z_log_var = Dense(latent_dim, name='z_log_var')(inter_x3)\n",
    "\n",
    "# # use reparameterization trick to push the sampling out as input\n",
    "#     z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "#     encoder = Model(inputs_x, [z_mean, z_log_var,z], name='encoder')\n",
    "#     encoder.compile(optimizer = Adam(learning_rate= 0.001))\n",
    "#     encoder.compile(loss = vae_loss(0.5,inputs_k_pr, inputs_k,z_mean,z_log_var))\n",
    "\n",
    "\n",
    "#     return encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Dense(\n",
    "      hp.Choice('units', [8, 16, 32]),\n",
    "      activation='relu'))\n",
    "  model.add(keras.layers.Dense(1, activation='relu'))\n",
    "  model.compile(loss='mse')\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from .\\untitled_project\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "def generate_sin_samples(N = 1000, domain = 4):\n",
    "\n",
    "    x = (np.random.rand(N, 1)-0.5) * domain * np.pi\n",
    "    y = np.sin(x) + np.random.randn(N, 1) * 0.05\n",
    "\n",
    "    return x, y\n",
    "\n",
    "x,y = generate_sin_samples(N = 10000, domain = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.33, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
